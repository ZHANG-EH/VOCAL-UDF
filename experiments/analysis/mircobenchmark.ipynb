{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import yaml\n",
    "import random\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "from sklearn.metrics import f1_score\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import display, HTML\n",
    "config = yaml.safe_load(open(\"/gscratch/balazinska/enhaoz/VOCAL-UDF/configs/config.yaml\", \"r\"))\n",
    "\n",
    "import re\n",
    "matplotlib.rcParams['pdf.fonttype'] = 42\n",
    "matplotlib.rcParams['ps.fonttype'] = 42\n",
    "\n",
    "CB_color_cycle = ['#377eb8', '#ff7f00', '#4daf4a',\n",
    "                  '#f781bf', '#a65628', '#984ea3',\n",
    "                  '#999999', '#e41a1c', '#dede00']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NL To DSL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_nl_to_dsl(dataset, query_class_names, question_ids, run_ids, vocal_udf_config_name):\n",
    "    num_correct = 0\n",
    "    num_correct_98 = 0\n",
    "    num_total = 0\n",
    "    failed_scores = []\n",
    "    for query_class_name in query_class_names:\n",
    "        for run_id in run_ids:\n",
    "            for question_id in question_ids:\n",
    "                num_total += 1\n",
    "                try:\n",
    "                    with open(os.path.join(config['log_dir'], \"nl_to_dsl\", dataset, query_class_name, vocal_udf_config_name, f\"qid={question_id}-run={run_id}.log\"), \"r\") as f:\n",
    "                        lines = f.readlines()\n",
    "                    f1_score = -1\n",
    "                    for line in lines:\n",
    "                        if \"F1 score:\" in line:\n",
    "                            f1_score_pattern = r\"F1 score: ([0-9.]+)\"\n",
    "                            match = re.search(f1_score_pattern, line)\n",
    "                            f1_score = float(match.group(1))\n",
    "                            break\n",
    "                    if f1_score == -1:\n",
    "                        print(f\"failed task: qid={question_id}-run={run_id}\")\n",
    "                        f1_score = 0\n",
    "                    if f1_score == 1:\n",
    "                        num_correct += 1\n",
    "                    else:\n",
    "                        failed_scores.append(f1_score)\n",
    "                        if f1_score < 0.98:\n",
    "                            print(f\"failed task: qid={question_id}-run={run_id}\")\n",
    "                    if f1_score >= 0.98:\n",
    "                        if f1_score < 1:\n",
    "                            print(f\"correct (1>f1>=0.98) task: qid={question_id}-run={run_id}\")\n",
    "                        num_correct_98 += 1\n",
    "                except Exception as e:\n",
    "                    print(e)\n",
    "    print(f\"num_total={num_total}, num_correct={num_correct/num_total}, num_correct_98={num_correct_98/num_total}\")\n",
    "    failed_scores.sort(reverse = True)\n",
    "    print(f\"failed_scores={failed_scores}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "correct (1>f1>=0.98) task: qid=1-run=0\n",
      "correct (1>f1>=0.98) task: qid=2-run=0\n",
      "correct (1>f1>=0.98) task: qid=3-run=0\n",
      "failed task: qid=5-run=0\n",
      "correct (1>f1>=0.98) task: qid=6-run=0\n",
      "failed task: qid=8-run=0\n",
      "correct (1>f1>=0.98) task: qid=9-run=0\n",
      "correct (1>f1>=0.98) task: qid=19-run=0\n",
      "correct (1>f1>=0.98) task: qid=1-run=1\n",
      "correct (1>f1>=0.98) task: qid=2-run=1\n",
      "correct (1>f1>=0.98) task: qid=3-run=1\n",
      "failed task: qid=5-run=1\n",
      "correct (1>f1>=0.98) task: qid=6-run=1\n",
      "failed task: qid=8-run=1\n",
      "correct (1>f1>=0.98) task: qid=9-run=1\n",
      "correct (1>f1>=0.98) task: qid=1-run=2\n",
      "correct (1>f1>=0.98) task: qid=2-run=2\n",
      "correct (1>f1>=0.98) task: qid=3-run=2\n",
      "failed task: qid=5-run=2\n",
      "correct (1>f1>=0.98) task: qid=6-run=2\n",
      "failed task: qid=8-run=2\n",
      "correct (1>f1>=0.98) task: qid=9-run=2\n",
      "num_total=90, num_correct=0.7555555555555555, num_correct_98=0.9333333333333333\n",
      "failed_scores=[0.9994895354772844, 0.9994895354772844, 0.9994895354772844, 0.9993861264579497, 0.9993861264579497, 0.9993861264579497, 0.9991673605328892, 0.9991673605328892, 0.9991673605328892, 0.9991659716430359, 0.9991659716430359, 0.9991659716430359, 0.9982847341337907, 0.9982847341337907, 0.9982847341337907, 0.98989898989899, 0.935687263556116, 0.935687263556116, 0.6556016597510373, 0.5884988797610157, 0.5884988797610157, 0.5884988797610157]\n"
     ]
    }
   ],
   "source": [
    "dataset = \"clevrer\"\n",
    "query_class_names = [\n",
    "    \"3_new_udfs_labels\",\n",
    "]\n",
    "question_ids = list(range(30))\n",
    "run_ids = list(range(3))\n",
    "vocal_udf_config_name = \"ninterp=10-nparams=5-kwargs=True-pixels=True-pretrained_models=False-ntrain_distill=100-nselection_samples=500-selection=both-labels=user-budget=20-llm_method=gpt4v\"\n",
    "\n",
    "eval_nl_to_dsl(dataset, query_class_names, question_ids, run_ids, vocal_udf_config_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "correct (1>f1>=0.98) task: qid=1-run=0\n",
      "failed task: qid=8-run=0\n",
      "correct (1>f1>=0.98) task: qid=1-run=1\n",
      "correct (1>f1>=0.98) task: qid=1-run=2\n",
      "failed task: qid=3-run=0\n",
      "correct (1>f1>=0.98) task: qid=6-run=0\n",
      "failed task: qid=3-run=1\n",
      "correct (1>f1>=0.98) task: qid=6-run=1\n",
      "failed task: qid=3-run=2\n",
      "correct (1>f1>=0.98) task: qid=6-run=2\n",
      "num_total=90, num_correct=0.8888888888888888, num_correct_98=0.9555555555555556\n",
      "failed_scores=[0.9925925925925926, 0.9925925925925926, 0.9925925925925926, 0.9868766404199476, 0.9868766404199476, 0.9868766404199476, 0.9217391304347825, 0.8636363636363636, 0.8636363636363636, 0.8636363636363636]\n"
     ]
    }
   ],
   "source": [
    "dataset = \"cityflow\"\n",
    "query_class_names = [\n",
    "    \"unavailable_pred=1-unavailable_attr_pred=1-npred=1-nattr_pred=2-nvars=3-depth=3-max_duration=15-min_npos=74-max_npos=737\",\n",
    "    \"unavailable_pred=1-unavailable_attr_pred=1-npred=2-nattr_pred=2-nvars=3-depth=3-max_duration=15-min_npos=74-max_npos=737\"\n",
    "]\n",
    "question_ids = list(range(15))\n",
    "run_ids = list(range(3))\n",
    "vocal_udf_config_name = \"ninterp=10-nparams=5-kwargs=True-pixels=False-pretrained_models=False-ntrain_distill=500-nselection_samples=500-selection=both-labels=user-budget=50-llm_method=gpt4v\"\n",
    "\n",
    "eval_nl_to_dsl(dataset, query_class_names, question_ids, run_ids, vocal_udf_config_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "correct (1>f1>=0.98) task: qid=2-run=0\n",
      "correct (1>f1>=0.98) task: qid=2-run=1\n",
      "correct (1>f1>=0.98) task: qid=2-run=2\n",
      "failed task: qid=0-run=1\n",
      "failed task: qid=0-run=2\n",
      "failed task: qid=2-run=0\n",
      "correct (1>f1>=0.98) task: qid=4-run=0\n",
      "correct (1>f1>=0.98) task: qid=7-run=0\n",
      "correct (1>f1>=0.98) task: qid=2-run=1\n",
      "correct (1>f1>=0.98) task: qid=4-run=1\n",
      "correct (1>f1>=0.98) task: qid=7-run=1\n",
      "correct (1>f1>=0.98) task: qid=2-run=2\n",
      "correct (1>f1>=0.98) task: qid=4-run=2\n",
      "correct (1>f1>=0.98) task: qid=7-run=2\n",
      "num_total=90, num_correct=0.8444444444444444, num_correct_98=0.9666666666666667\n",
      "failed_scores=[0.9947368421052631, 0.9947368421052631, 0.9909297052154195, 0.9909297052154195, 0.9904153354632589, 0.9904153354632589, 0.9904153354632589, 0.9852302345786272, 0.9850107066381156, 0.9850107066381156, 0.9850107066381156, 0.9563164108618654, 0.23019250253292803, 0.23019250253292803]\n"
     ]
    }
   ],
   "source": [
    "dataset = \"charades\"\n",
    "query_class_names = [\n",
    "    \"unavailable=2-npred=4-nobj_pred=1-nvars=3-depth=2\",\n",
    "    \"unavailable=2-npred=4-nobj_pred=1-nvars=2-depth=2\",\n",
    "    \"unavailable=2-npred=3-nobj_pred=1-nvars=2-depth=2\"\n",
    "]\n",
    "question_ids = list(range(10))\n",
    "run_ids = list(range(3))\n",
    "vocal_udf_config_name = \"ninterp=10-nparams=5-kwargs=True-pixels=False-pretrained_models=False-ntrain_distill=500-nselection_samples=500-selection=both-labels=user-budget=50-llm_method=gpt4v\"\n",
    "\n",
    "eval_nl_to_dsl(dataset, query_class_names, question_ids, run_ids, vocal_udf_config_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Proposing UDFs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def standardize_udf_name(dataset, udf_name):\n",
    "    udf_name = udf_name.replace(\" \", \"\").replace(\"_\", \"\").lower()\n",
    "    # location_bottom_left, behind_and_near, in_front_of, behind_left_of, far_left_of, behind_and_left_of, near_and_right_of,\n",
    "    if dataset == \"clevrer\":\n",
    "        if udf_name in [\"materialmetal\"]:\n",
    "            udf_name = \"metal\"\n",
    "        elif udf_name in [\"shapecylinder\"]:\n",
    "            udf_name = \"cylinder\"\n",
    "        elif udf_name in [\"coloryellow\"]:\n",
    "            udf_name = \"yellow\"\n",
    "        elif udf_name in [\"colorpurple\"]:\n",
    "            udf_name = \"purple\"\n",
    "        elif udf_name in [\"colorcyan\"]:\n",
    "            udf_name = \"cyan\"\n",
    "        elif udf_name in [\"colorbrown\"]:\n",
    "            udf_name = \"brown\"\n",
    "        elif udf_name in [\"locationright\"]:\n",
    "            udf_name = \"right\"\n",
    "        elif udf_name in [\"locationbottom\"]:\n",
    "            udf_name = \"bottom\"\n",
    "        elif udf_name in [\"farfrom\", \"farawayfrom\"]:\n",
    "            udf_name = \"far\"\n",
    "        elif udf_name in [\"nearof\"]:\n",
    "            udf_name = \"near\"\n",
    "        elif udf_name in [\"behindof\"]:\n",
    "            udf_name = \"behind\"\n",
    "        elif udf_name in [\"shapecylindrical\"]:\n",
    "            udf_name = \"cylinder\"\n",
    "    elif dataset == \"cityflow\":\n",
    "        # right_of, left_of, pickup, in_front_of_white, suv_and_red, white_sedan, moves_in_front_of, color_blue, color_red,\n",
    "        if udf_name in [\"colorred\"]:\n",
    "            udf_name = \"red\"\n",
    "        elif udf_name in [\"colorblue\"]:\n",
    "            udf_name = \"blue\"\n",
    "        elif udf_name in [\"rightof\"]:\n",
    "            udf_name = \"totherightof\"\n",
    "        elif udf_name in [\"leftof\"]:\n",
    "            udf_name = \"totheleftof\"\n",
    "        elif udf_name in [\"pickup\"]:\n",
    "            udf_name = \"pickuptruck\"\n",
    "    elif dataset == \"charades\":\n",
    "        # inside, inside_of, eating_from, inside_and_interacting_with, inside_while_drinking_from, inside_while_drinking, drinking_from_inside, beneath_and_wearing, moving_behind\n",
    "        if udf_name in [\"inside\", \"insideof\"]:\n",
    "            udf_name = \"in\"\n",
    "        elif udf_name in [\"eatingfrom\"]:\n",
    "            udf_name = \"eating\"\n",
    "    return udf_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_proposing_udfs(dataset, query_class_names, num_new_udf_list, question_ids, run_ids, vocal_udf_config_name):\n",
    "    # Is the system able to propose UDFs when needed?\n",
    "    # When does our approach work? When does it fail?\n",
    "    # What UDFs does the system propose?\n",
    "    # Same 90 queries, analyze how well the proposed UDFs match with the ground-truth\n",
    "    # 1. For every proposed UDF, how to define \"match\"?\n",
    "    # 2. When does it over-proposes?\n",
    "    # 3. When does it under-proposes?\n",
    "    # 4. What's the average number of UDFs proposed?\n",
    "    FP_list = defaultdict(int) # proposed UDFs\n",
    "    FN_list = defaultdict(int) # gt UDFs\n",
    "    num_proposed_udfs = 0\n",
    "    num_gt_new_udfs = 0\n",
    "    for num_new_udfs in num_new_udf_list:\n",
    "        avg_num_proposed_udfs = []\n",
    "        for query_class_name in query_class_names:\n",
    "            for run_id in run_ids:\n",
    "                for question_id in question_ids:\n",
    "                    proposed_udfs = []\n",
    "                    try:\n",
    "                        with open(os.path.join(config['output_dir'], \"udf_generation\", dataset, query_class_name, f\"num_missing_udfs={num_new_udfs}\", vocal_udf_config_name, f\"qid={question_id}-run={run_id}.json\"), \"r\") as f:\n",
    "                            data = json.load(f)\n",
    "                        proposed_udfs.extend(data[\"on_the_fly_udf_names\"])\n",
    "                        proposed_udfs.extend(data[\"materialized_df_names\"])\n",
    "                    except Exception as e:\n",
    "                        print(e)\n",
    "                    if len(proposed_udfs) > 0 and num_new_udfs == 0:\n",
    "                        print(f\"query_class_name={query_class_name}, question_id={question_id}, run_id={run_id}\")\n",
    "                    num_proposed_udfs += len(proposed_udfs)\n",
    "                    num_gt_new_udfs += num_new_udfs\n",
    "                    avg_num_proposed_udfs.append(len(proposed_udfs))\n",
    "                    input_query_file = os.path.join(config[\"data_dir\"], dataset, f\"{query_class_name}.json\")\n",
    "                    input_query = json.load(open(input_query_file, \"r\"))[\"questions\"][question_id]\n",
    "                    new_modules = input_query[\"new_modules\"]\n",
    "                    gt_udfs = new_modules[(len(new_modules) - num_new_udfs):]\n",
    "                    for proposed_udf in proposed_udfs:\n",
    "                        if standardize_udf_name(dataset, proposed_udf) not in [gt_udf.replace(\" \", \"\").replace(\"_\", \"\").lower() for gt_udf in gt_udfs]:\n",
    "                            FP_list[proposed_udf] += 1\n",
    "\n",
    "                    for gt_udf in gt_udfs:\n",
    "                        if gt_udf.replace(\" \", \"\").replace(\"_\", \"\").lower() not in [standardize_udf_name(dataset, proposed_udf) for proposed_udf in proposed_udfs]:\n",
    "                            FN_list[gt_udf] += 1\n",
    "                            if gt_udf == \"HOLDING\":\n",
    "                                print(f\"FN: query_class_name={query_class_name}, question_id={question_id}, run_id={run_id}\")\n",
    "\n",
    "        avg_num_proposed_udfs = np.mean(avg_num_proposed_udfs)\n",
    "        print(f\"num_new_udfs={num_new_udfs}: {avg_num_proposed_udfs}\")\n",
    "\n",
    "    # FP: over-proposed\n",
    "    # FN: under-proposed\n",
    "    print(\"FP: over-proposed\")\n",
    "    for udf_name, count in sorted(FP_list.items(), key=lambda x: -x[1]):\n",
    "        print(f\"{udf_name}: {count}\")\n",
    "    print(f\"#FP={sum(FP_list.values())}\")\n",
    "    print(\"FN: under-proposed\")\n",
    "    for udf_name, count in sorted(FN_list.items(), key=lambda x: -x[1]):\n",
    "        print(f\"{udf_name}: {count}\")\n",
    "    print(f\"#FN={sum(FN_list.values())}\")\n",
    "    print(\"num_proposed_udfs:\", num_proposed_udfs)\n",
    "    print(\"num_gt_new_udfs:\", num_gt_new_udfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_new_udfs=0: 0.0\n",
      "num_new_udfs=1: 0.6222222222222222\n",
      "[Errno 2] No such file or directory: '/gscratch/balazinska/enhaoz/VOCAL-UDF/outputs/udf_generation/clevrer/3_new_udfs_labels/num_missing_udfs=2/ninterp=10-nparams=5-kwargs=True-pixels=True-pretrained_models=False-ntrain_distill=100-nselection_samples=500-selection=both-labels=user-budget=20-llm_method=gpt4v/qid=26-run=1.json'\n",
      "num_new_udfs=2: 1.6777777777777778\n",
      "num_new_udfs=3: 2.7666666666666666\n",
      "FP: over-proposed\n",
      "location_bottom_left: 7\n",
      "behind_and_near: 4\n",
      "location_right: 4\n",
      "in_front_of: 3\n",
      "behind_left_of: 2\n",
      "far_left_of: 1\n",
      "behind_and_left_of: 1\n",
      "near_and_right_of: 1\n",
      "#FP=23\n",
      "FN: under-proposed\n",
      "RIGHTOF: 36\n",
      "BEHIND: 32\n",
      "FAR: 12\n",
      "RIGHT: 10\n",
      "NEAR: 9\n",
      "BOTTOM: 7\n",
      "CYLINDER: 1\n",
      "#FN=107\n",
      "num_proposed_udfs: 456\n",
      "num_gt_new_udfs: 540\n"
     ]
    }
   ],
   "source": [
    "dataset = \"clevrer\"\n",
    "query_class_names = [\n",
    "    \"3_new_udfs_labels\",\n",
    "]\n",
    "question_ids = list(range(30))\n",
    "run_ids = list(range(3))\n",
    "num_new_udf_list = [0, 1, 2, 3]\n",
    "vocal_udf_config_name = \"ninterp=10-nparams=5-kwargs=True-pixels=True-pretrained_models=False-ntrain_distill=100-nselection_samples=500-selection=both-labels=user-budget=20-llm_method=gpt4v\"\n",
    "\n",
    "eval_proposing_udfs(dataset, query_class_names, num_new_udf_list, question_ids, run_ids, vocal_udf_config_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_new_udfs=0: 0.0\n",
      "num_new_udfs=1: 1.011111111111111\n",
      "num_new_udfs=2: 1.9555555555555555\n",
      "FP: over-proposed\n",
      "in_front_of_white: 1\n",
      "suv_and_red: 1\n",
      "white_sedan: 1\n",
      "moves_in_front_of: 1\n",
      "#FP=4\n",
      "FN: under-proposed\n",
      "INFRONTOF: 5\n",
      "BLACK: 1\n",
      "TOTHERIGHTOF: 1\n",
      "#FN=7\n",
      "num_proposed_udfs: 267\n",
      "num_gt_new_udfs: 270\n"
     ]
    }
   ],
   "source": [
    "dataset = \"cityflow\"\n",
    "query_class_names = [\n",
    "    \"unavailable_pred=1-unavailable_attr_pred=1-npred=1-nattr_pred=2-nvars=3-depth=3-max_duration=15-min_npos=74-max_npos=737\",\n",
    "    \"unavailable_pred=1-unavailable_attr_pred=1-npred=2-nattr_pred=2-nvars=3-depth=3-max_duration=15-min_npos=74-max_npos=737\"\n",
    "]\n",
    "question_ids = list(range(15))\n",
    "run_ids = list(range(3))\n",
    "num_new_udf_list = [0, 1, 2]\n",
    "vocal_udf_config_name = \"ninterp=10-nparams=5-kwargs=True-pixels=False-pretrained_models=False-ntrain_distill=500-nselection_samples=500-selection=both-labels=user-budget=50-llm_method=gpt4v\"\n",
    "\n",
    "eval_proposing_udfs(dataset, query_class_names, num_new_udf_list, question_ids, run_ids, vocal_udf_config_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "query_class_name=unavailable=2-npred=4-nobj_pred=1-nvars=3-depth=2, question_id=2, run_id=2\n",
      "query_class_name=unavailable=2-npred=4-nobj_pred=1-nvars=2-depth=2, question_id=0, run_id=0\n",
      "query_class_name=unavailable=2-npred=4-nobj_pred=1-nvars=2-depth=2, question_id=0, run_id=1\n",
      "query_class_name=unavailable=2-npred=4-nobj_pred=1-nvars=2-depth=2, question_id=0, run_id=2\n",
      "num_new_udfs=0: 0.044444444444444446\n",
      "FN: query_class_name=unavailable=2-npred=4-nobj_pred=1-nvars=3-depth=2, question_id=4, run_id=0\n",
      "FN: query_class_name=unavailable=2-npred=4-nobj_pred=1-nvars=3-depth=2, question_id=6, run_id=0\n",
      "FN: query_class_name=unavailable=2-npred=4-nobj_pred=1-nvars=3-depth=2, question_id=7, run_id=0\n",
      "FN: query_class_name=unavailable=2-npred=4-nobj_pred=1-nvars=3-depth=2, question_id=4, run_id=1\n",
      "FN: query_class_name=unavailable=2-npred=4-nobj_pred=1-nvars=3-depth=2, question_id=6, run_id=1\n",
      "FN: query_class_name=unavailable=2-npred=4-nobj_pred=1-nvars=3-depth=2, question_id=4, run_id=2\n",
      "FN: query_class_name=unavailable=2-npred=4-nobj_pred=1-nvars=2-depth=2, question_id=4, run_id=0\n",
      "FN: query_class_name=unavailable=2-npred=4-nobj_pred=1-nvars=2-depth=2, question_id=4, run_id=1\n",
      "FN: query_class_name=unavailable=2-npred=4-nobj_pred=1-nvars=2-depth=2, question_id=4, run_id=2\n",
      "num_new_udfs=1: 0.7444444444444445\n",
      "FN: query_class_name=unavailable=2-npred=4-nobj_pred=1-nvars=3-depth=2, question_id=2, run_id=0\n",
      "FN: query_class_name=unavailable=2-npred=4-nobj_pred=1-nvars=3-depth=2, question_id=6, run_id=0\n",
      "FN: query_class_name=unavailable=2-npred=4-nobj_pred=1-nvars=3-depth=2, question_id=7, run_id=0\n",
      "FN: query_class_name=unavailable=2-npred=4-nobj_pred=1-nvars=3-depth=2, question_id=6, run_id=2\n",
      "num_new_udfs=2: 1.8222222222222222\n",
      "FP: over-proposed\n",
      "inside: 3\n",
      "inside_and_interacting_with: 1\n",
      "inside_while_drinking_from: 1\n",
      "inside_while_drinking: 1\n",
      "drinking_from_inside: 1\n",
      "beneath_and_wearing: 1\n",
      "moving_behind: 1\n",
      "#FP=9\n",
      "FN: under-proposed\n",
      "BEHIND: 15\n",
      "HOLDING: 13\n",
      "BENEATH: 5\n",
      "SITTINGON: 3\n",
      "CARRYING: 3\n",
      "IN: 3\n",
      "STANDINGON: 2\n",
      "#FN=44\n",
      "num_proposed_udfs: 235\n",
      "num_gt_new_udfs: 270\n"
     ]
    }
   ],
   "source": [
    "dataset = \"charades\"\n",
    "query_class_names = [\n",
    "    \"unavailable=2-npred=4-nobj_pred=1-nvars=3-depth=2\",\n",
    "    \"unavailable=2-npred=4-nobj_pred=1-nvars=2-depth=2\",\n",
    "    \"unavailable=2-npred=3-nobj_pred=1-nvars=2-depth=2\"\n",
    "]\n",
    "question_ids = list(range(10))\n",
    "run_ids = list(range(3))\n",
    "num_new_udf_list = [0, 1, 2]\n",
    "vocal_udf_config_name = \"ninterp=10-nparams=5-kwargs=True-pixels=False-pretrained_models=False-ntrain_distill=500-nselection_samples=500-selection=both-labels=user-budget=50-llm_method=gpt4v\"\n",
    "\n",
    "eval_proposing_udfs(dataset, query_class_names, num_new_udf_list, question_ids, run_ids, vocal_udf_config_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Selection Strategy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_correctly_proposed(dataset, udf_name, gt_udfs):\n",
    "    udf_name = standardize_udf_name(dataset, udf_name)\n",
    "    if udf_name not in [gt_udf.replace(\" \", \"\").replace(\"_\", \"\").lower() for gt_udf in gt_udfs]:\n",
    "        return False\n",
    "    else:\n",
    "        return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_selection_strategy(dataset, query_class_names, question_ids, run_ids, vocal_udf_config_name):\n",
    "    \"\"\" Only evaluate correctly proposed UDFs\"\"\"\n",
    "    num_incorrect_proposal = 0\n",
    "    # UDF selection stats\n",
    "    num_correct_selection = 0\n",
    "    num_correct_selection_80 = 0\n",
    "    num_incorrect_selection = 0\n",
    "\n",
    "    # Choosing between UDF types stats (when 'dummy' is the only best UDF type)\n",
    "    num_correct_udf_type_dummy = 0 # The number of times the selected UDF type is correct\n",
    "    num_incorrect_udf_type_dummy = 0 # The number of times the selected UDF type is incorrect\n",
    "    num_llm_decides_program_udf_type_dummy = 0 # The number of times the LLM-selected UDF type is program\n",
    "    num_llm_decides_model_udf_type_dummy = 0 # The number of times the LLM-selected UDF type is model\n",
    "\n",
    "    # Choosing between UDF types stats (when the best UDF types are not 'dummy')\n",
    "    num_correct_udf_type_not_dummy = 0\n",
    "    num_incorrect_udf_type_not_dummy = 0\n",
    "    num_llm_decides_correct_udf_type_not_dummy = 0\n",
    "    num_llm_decides_incorrect_udf_type_not_dummy = 0\n",
    "\n",
    "    for query_class_name in query_class_names:\n",
    "        for run_id in run_ids:\n",
    "            for question_id in question_ids:\n",
    "                try:\n",
    "                    input_query_file = os.path.join(config[\"data_dir\"], dataset, f\"{query_class_name}.json\")\n",
    "                    input_query = json.load(open(input_query_file, \"r\"))[\"questions\"][question_id]\n",
    "                    new_modules = input_query[\"new_modules\"]\n",
    "                    gt_udfs = new_modules\n",
    "\n",
    "                    with open(os.path.join(config['output_dir'], \"best_udf_type\", dataset, query_class_name, vocal_udf_config_name, f\"qid={question_id}-run={run_id}.json\"), \"r\") as f:\n",
    "                        best_udf_type_data = json.load(f)\n",
    "                    for udf_name, v in best_udf_type_data.items():\n",
    "                        if not is_correctly_proposed(dataset, udf_name, gt_udfs):\n",
    "                            num_incorrect_proposal += 1\n",
    "                            continue\n",
    "                        if v[\"best_test_score\"] == v[\"selected_test_score\"]:\n",
    "                            num_correct_selection += 1\n",
    "                        else:\n",
    "                            num_incorrect_selection += 1\n",
    "\n",
    "                        if v[\"selected_test_score\"] >= 0.8 * v[\"best_test_score\"]:\n",
    "                            num_correct_selection_80 += 1\n",
    "\n",
    "                        if \"dummy\" not in v[\"best_udf_types\"] or (\"dummy\" in v[\"best_udf_types\"] and len(v[\"best_udf_types\"]) > 1):\n",
    "                            if v[\"selected_udf_type\"] in v[\"best_udf_types\"]:\n",
    "                                num_correct_udf_type_not_dummy += 1\n",
    "                            else:\n",
    "                                num_incorrect_udf_type_not_dummy += 1\n",
    "                        else:\n",
    "                            assert len(v[\"best_udf_types\"]) == 1, \"assert 1\"\n",
    "                            if v[\"selected_udf_type\"] == \"dummy\":\n",
    "                                num_correct_udf_type_dummy += 1\n",
    "                            else:\n",
    "                                num_incorrect_udf_type_dummy += 1\n",
    "                    with open(os.path.join(config['output_dir'], \"llm_decides_udf_type\", dataset, query_class_name, vocal_udf_config_name, f\"qid={question_id}-run={run_id}.json\"), \"r\") as f:\n",
    "                        llm_decides_udf_type_data = json.load(f)\n",
    "                    for udf_name, llm_decides_udf_type in llm_decides_udf_type_data.items():\n",
    "                        if not is_correctly_proposed(dataset, udf_name, gt_udfs):\n",
    "                            continue\n",
    "                        if \"dummy\" not in best_udf_type_data[udf_name][\"best_udf_types\"] or (\"dummy\" in best_udf_type_data[udf_name][\"best_udf_types\"] and len(best_udf_type_data[udf_name][\"best_udf_types\"]) > 1):\n",
    "                            if llm_decides_udf_type in best_udf_type_data[udf_name][\"best_udf_types\"]:\n",
    "                                num_llm_decides_correct_udf_type_not_dummy += 1\n",
    "                            else:\n",
    "                                num_llm_decides_incorrect_udf_type_not_dummy += 1\n",
    "                        else:\n",
    "                            if llm_decides_udf_type == \"program\":\n",
    "                                num_llm_decides_program_udf_type_dummy += 1\n",
    "                            elif llm_decides_udf_type == \"model\":\n",
    "                                num_llm_decides_model_udf_type_dummy += 1\n",
    "                            else:\n",
    "                                raise ValueError(f\"llm_decides_udf_type={llm_decides_udf_type}\")\n",
    "                except Exception as e:\n",
    "                    print(f\"Error: {e}, query_class_name={query_class_name}, question_id={question_id}, run_id={run_id}\")\n",
    "\n",
    "    print(f\"num_incorrect_proposal={num_incorrect_proposal}\")\n",
    "\n",
    "    print(f\"num_correct_selection={num_correct_selection}, num_incorrect_selection={num_incorrect_selection}, ratio={num_correct_selection/(num_correct_selection+num_incorrect_selection)}, num_correct_selection_80={num_correct_selection_80}, ratio={num_correct_selection_80/(num_correct_selection+num_incorrect_selection)}\")\n",
    "    print()\n",
    "\n",
    "    print(\"[When the best UDF types are not 'dummy']\")\n",
    "    print(f\"num_correct_udf_type_not_dummy={num_correct_udf_type_not_dummy}, num_incorrect_udf_type_not_dummy={num_incorrect_udf_type_not_dummy}, ratio={num_correct_udf_type_not_dummy/(num_correct_udf_type_not_dummy+num_incorrect_udf_type_not_dummy)}\")\n",
    "    print(f\"num_llm_decides_correct_udf_type_not_dummy={num_llm_decides_correct_udf_type_not_dummy}, num_llm_decides_incorrect_udf_type_not_dummy={num_llm_decides_incorrect_udf_type_not_dummy}, ratio={num_llm_decides_correct_udf_type_not_dummy/(num_llm_decides_correct_udf_type_not_dummy+num_llm_decides_incorrect_udf_type_not_dummy)}\")\n",
    "    print()\n",
    "\n",
    "    print(\"[When 'dummy' is the only best UDF type]\")\n",
    "    print(f\"num_correct_udf_type_dummy={num_correct_udf_type_dummy}, num_incorrect_udf_type_dummy={num_incorrect_udf_type_dummy}, ratio={num_correct_udf_type_dummy/(num_correct_udf_type_dummy+num_incorrect_udf_type_dummy)}\")\n",
    "    print(f\"num_llm_decides_program_udf_type_dummy={num_llm_decides_program_udf_type_dummy}, num_llm_decides_model_udf_type_dummy={num_llm_decides_model_udf_type_dummy}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_incorrect_proposal=7\n",
      "num_correct_selection=141, num_incorrect_selection=101, ratio=0.5826446280991735, num_correct_selection_80=221, ratio=0.9132231404958677\n",
      "\n",
      "[When the best UDF types are not 'dummy']\n",
      "num_correct_udf_type_not_dummy=212, num_incorrect_udf_type_not_dummy=21, ratio=0.9098712446351931\n",
      "num_llm_decides_correct_udf_type_not_dummy=164, num_llm_decides_incorrect_udf_type_not_dummy=69, ratio=0.703862660944206\n",
      "\n",
      "[When 'dummy' is the only best UDF type]\n",
      "num_correct_udf_type_dummy=5, num_incorrect_udf_type_dummy=4, ratio=0.5555555555555556\n",
      "num_llm_decides_program_udf_type_dummy=5, num_llm_decides_model_udf_type_dummy=4\n"
     ]
    }
   ],
   "source": [
    "dataset = \"clevrer\"\n",
    "query_class_names = [\n",
    "    \"3_new_udfs_labels\",\n",
    "]\n",
    "question_ids = list(range(30))\n",
    "run_ids = list(range(3))\n",
    "vocal_udf_config_name = \"ninterp=10-nparams=5-kwargs=True-pixels=True-pretrained_models=False-ntrain_distill=100-nselection_samples=500-selection=both-labels=user-budget=20-llm_method=gpt4v\"\n",
    "\n",
    "eval_selection_strategy(dataset, query_class_names, question_ids, run_ids, vocal_udf_config_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_incorrect_proposal=0\n",
      "num_correct_selection=130, num_incorrect_selection=46, ratio=0.7386363636363636, num_correct_selection_80=148, ratio=0.8409090909090909\n",
      "\n",
      "[When the best UDF types are not 'dummy']\n",
      "num_correct_udf_type_not_dummy=144, num_incorrect_udf_type_not_dummy=30, ratio=0.8275862068965517\n",
      "num_llm_decides_correct_udf_type_not_dummy=114, num_llm_decides_incorrect_udf_type_not_dummy=60, ratio=0.6551724137931034\n",
      "\n",
      "[When 'dummy' is the only best UDF type]\n",
      "num_correct_udf_type_dummy=1, num_incorrect_udf_type_dummy=1, ratio=0.5\n",
      "num_llm_decides_program_udf_type_dummy=1, num_llm_decides_model_udf_type_dummy=1\n"
     ]
    }
   ],
   "source": [
    "dataset = \"cityflow\"\n",
    "query_class_names = [\n",
    "    \"unavailable_pred=1-unavailable_attr_pred=1-npred=1-nattr_pred=2-nvars=3-depth=3-max_duration=15-min_npos=74-max_npos=737\",\n",
    "    \"unavailable_pred=1-unavailable_attr_pred=1-npred=2-nattr_pred=2-nvars=3-depth=3-max_duration=15-min_npos=74-max_npos=737\"\n",
    "]\n",
    "question_ids = list(range(15))\n",
    "run_ids = list(range(3))\n",
    "vocal_udf_config_name = \"ninterp=10-nparams=5-kwargs=True-pixels=False-pretrained_models=False-ntrain_distill=500-nselection_samples=500-selection=both-labels=user-budget=50-llm_method=gpt4v\"\n",
    "\n",
    "eval_selection_strategy(dataset, query_class_names, question_ids, run_ids, vocal_udf_config_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_incorrect_proposal=0\n",
      "num_correct_selection=110, num_incorrect_selection=54, ratio=0.6707317073170732, num_correct_selection_80=137, ratio=0.8353658536585366\n",
      "\n",
      "[When the best UDF types are not 'dummy']\n",
      "num_correct_udf_type_not_dummy=92, num_incorrect_udf_type_not_dummy=31, ratio=0.7479674796747967\n",
      "num_llm_decides_correct_udf_type_not_dummy=76, num_llm_decides_incorrect_udf_type_not_dummy=47, ratio=0.6178861788617886\n",
      "\n",
      "[When 'dummy' is the only best UDF type]\n",
      "num_correct_udf_type_dummy=28, num_incorrect_udf_type_dummy=13, ratio=0.6829268292682927\n",
      "num_llm_decides_program_udf_type_dummy=7, num_llm_decides_model_udf_type_dummy=34\n"
     ]
    }
   ],
   "source": [
    "dataset = \"charades\"\n",
    "query_class_names = [\n",
    "    \"unavailable=2-npred=4-nobj_pred=1-nvars=3-depth=2\",\n",
    "    \"unavailable=2-npred=4-nobj_pred=1-nvars=2-depth=2\",\n",
    "    \"unavailable=2-npred=3-nobj_pred=1-nvars=2-depth=2\"\n",
    "]\n",
    "question_ids = list(range(10))\n",
    "run_ids = list(range(3))\n",
    "vocal_udf_config_name = \"ninterp=10-nparams=5-kwargs=True-pixels=False-pretrained_models=False-ntrain_distill=500-nselection_samples=500-selection=both-labels=user-budget=50-llm_method=gpt4v\"\n",
    "\n",
    "eval_selection_strategy(dataset, query_class_names, question_ids, run_ids, vocal_udf_config_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# UDF type stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_udf_type_stats(dataset, query_class_names, question_ids, run_ids, vocal_udf_config_name):\n",
    "    num_missing_udfs = 3 if dataset == \"clevrer\" else 2\n",
    "    num_dummy, num_program, num_model = 0, 0, 0\n",
    "    dummy_names = []\n",
    "    num_files = 0\n",
    "    for query_class_name in query_class_names:\n",
    "        for run_id in run_ids:\n",
    "            for question_id in question_ids:\n",
    "                try:\n",
    "                    input_query_file = os.path.join(config[\"data_dir\"], dataset, f\"{query_class_name}.json\")\n",
    "                    input_query = json.load(open(input_query_file, \"r\"))[\"questions\"][question_id]\n",
    "                    new_modules = input_query[\"new_modules\"]\n",
    "                    gt_udfs = new_modules\n",
    "                    with open(os.path.join(config['output_dir'], \"udf_generation\", dataset, query_class_name, f\"num_missing_udfs={num_missing_udfs}\", vocal_udf_config_name, f\"qid={question_id}-run={run_id}.json\"), \"r\") as f:\n",
    "                        data = json.load(f)\n",
    "                    num_files += 1\n",
    "                    for udf in data[\"registered_functions\"]:\n",
    "                        if \"semantic_interpretation\" in udf:\n",
    "                            udf_name = udf[\"signature\"].split(\"(\")[0]\n",
    "                            if not is_correctly_proposed(dataset, udf_name, gt_udfs):\n",
    "                                continue\n",
    "                            if udf[\"semantic_interpretation\"] == \"dummy\":\n",
    "                                num_dummy += 1\n",
    "                                dummy_names.append(udf_name)\n",
    "                            elif udf[\"semantic_interpretation\"] == \"model\":\n",
    "                                num_model += 1\n",
    "                            elif udf[\"function_implementation\"] != \"\":\n",
    "                                num_program += 1\n",
    "                            else:\n",
    "                                raise ValueError(f\"Unknown semantic_interpretation: {udf['semantic_interpretation']}\")\n",
    "                except Exception as e:\n",
    "                    print(f\"Error: {e}, query_class_name={query_class_name}, question_id={question_id}, run_id={run_id}\")\n",
    "    print(f\"num_program={num_program}, num_model={num_model}, num_dummy={num_dummy}, num_files={num_files}\")\n",
    "    num_all = num_dummy + num_program + num_model\n",
    "    print(f\"program percentage: {num_program / num_all:.2f}, model percentage: {num_model / num_all:.2f}, dummy percentage: {num_dummy / num_all:.2f}\")\n",
    "    print(f\"dummy names: {sorted(dummy_names)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clevrer\n",
      "num_program=172, num_model=55, num_dummy=15, num_files=90\n",
      "program percentage: 0.71, model percentage: 0.23, dummy percentage: 0.06\n",
      "dummy names: ['behind', 'behind', 'material_metal', 'material_metal', 'material_metal', 'material_metal', 'material_metal', 'material_metal', 'material_metal', 'material_metal', 'material_metal', 'material_metal', 'right_of', 'right_of', 'right_of']\n",
      "CityFlow\n",
      "num_program=100, num_model=64, num_dummy=12, num_files=90\n",
      "program percentage: 0.57, model percentage: 0.36, dummy percentage: 0.07\n",
      "dummy names: ['black', 'black', 'black', 'black', 'color_blue', 'in_front_of', 'in_front_of', 'in_front_of', 'left_of', 'left_of', 'left_of', 'sedan']\n",
      "Charades\n",
      "num_program=79, num_model=34, num_dummy=51, num_files=90\n",
      "program percentage: 0.48, model percentage: 0.21, dummy percentage: 0.31\n",
      "dummy names: ['behind', 'behind', 'behind', 'behind', 'behind', 'behind', 'behind', 'behind', 'behind', 'behind', 'behind', 'behind', 'behind', 'behind', 'behind', 'behind', 'behind', 'behind', 'behind', 'carrying', 'carrying', 'carrying', 'carrying', 'carrying', 'carrying', 'eating_from', 'holding', 'holding', 'holding', 'holding', 'holding', 'holding', 'inside', 'inside', 'inside', 'inside', 'inside', 'inside', 'inside', 'inside', 'inside', 'inside', 'inside', 'inside', 'inside', 'inside', 'inside', 'inside', 'inside_of', 'inside_of', 'standing_on']\n"
     ]
    }
   ],
   "source": [
    "# Clevrer\n",
    "print(\"Clevrer\")\n",
    "dataset = \"clevrer\"\n",
    "query_class_names = [\n",
    "    \"3_new_udfs_labels\",\n",
    "]\n",
    "question_ids = list(range(30))\n",
    "run_ids = list(range(3))\n",
    "vocal_udf_config_name = \"ninterp=10-nparams=5-kwargs=True-pixels=True-pretrained_models=False-ntrain_distill=100-nselection_samples=500-selection=both-labels=user-budget=20-llm_method=gpt4v\"\n",
    "eval_udf_type_stats(dataset, query_class_names, question_ids, run_ids, vocal_udf_config_name)\n",
    "\n",
    "# CityFlow\n",
    "print(\"CityFlow\")\n",
    "dataset = \"cityflow\"\n",
    "query_class_names = [\n",
    "    \"unavailable_pred=1-unavailable_attr_pred=1-npred=1-nattr_pred=2-nvars=3-depth=3-max_duration=15-min_npos=74-max_npos=737\",\n",
    "    \"unavailable_pred=1-unavailable_attr_pred=1-npred=2-nattr_pred=2-nvars=3-depth=3-max_duration=15-min_npos=74-max_npos=737\"\n",
    "]\n",
    "question_ids = list(range(15))\n",
    "run_ids = list(range(3))\n",
    "vocal_udf_config_name = \"ninterp=10-nparams=5-kwargs=True-pixels=False-pretrained_models=False-ntrain_distill=500-nselection_samples=500-selection=both-labels=user-budget=50-llm_method=gpt4v\"\n",
    "eval_udf_type_stats(dataset, query_class_names, question_ids, run_ids, vocal_udf_config_name)\n",
    "\n",
    "# Charades\n",
    "print(\"Charades\")\n",
    "dataset = \"charades\"\n",
    "query_class_names = [\n",
    "    \"unavailable=2-npred=4-nobj_pred=1-nvars=3-depth=2\",\n",
    "    \"unavailable=2-npred=4-nobj_pred=1-nvars=2-depth=2\",\n",
    "    \"unavailable=2-npred=3-nobj_pred=1-nvars=2-depth=2\"\n",
    "]\n",
    "question_ids = list(range(10))\n",
    "run_ids = list(range(3))\n",
    "vocal_udf_config_name = \"ninterp=10-nparams=5-kwargs=True-pixels=False-pretrained_models=False-ntrain_distill=500-nselection_samples=500-selection=both-labels=user-budget=50-llm_method=gpt4v\"\n",
    "eval_udf_type_stats(dataset, query_class_names, question_ids, run_ids, vocal_udf_config_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Program-based UDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_program_udfs(dataset, query_class_names, question_ids, run_ids, vocal_udf_config_name):\n",
    "    \"\"\" Only evaluate correctly proposed UDFs\"\"\"\n",
    "    best_program_types_when_best_is_program = defaultdict(int)\n",
    "    best_program_types_when_best_is_not_program = defaultdict(int)\n",
    "    f1_scores_when_best_is_program = []\n",
    "    f1_scores_when_best_is_not_program = []\n",
    "    best_f1_scores_when_best_is_program = []\n",
    "    best_f1_scores_when_best_is_not_program = []\n",
    "\n",
    "    for query_class_name in query_class_names:\n",
    "        for run_id in run_ids:\n",
    "            for question_id in question_ids:\n",
    "                try:\n",
    "                    input_query_file = os.path.join(config[\"data_dir\"], dataset, f\"{query_class_name}.json\")\n",
    "                    input_query = json.load(open(input_query_file, \"r\"))[\"questions\"][question_id]\n",
    "                    new_modules = input_query[\"new_modules\"]\n",
    "                    gt_udfs = new_modules\n",
    "\n",
    "                    with open(os.path.join(config['output_dir'], \"best_udf_type\", dataset, query_class_name, vocal_udf_config_name, f\"qid={question_id}-run={run_id}.json\"), \"r\") as f:\n",
    "                        best_udf_type_data = json.load(f)\n",
    "                    for udf_name, v in best_udf_type_data.items():\n",
    "                        if not is_correctly_proposed(dataset, udf_name, gt_udfs):\n",
    "                            continue\n",
    "                        candidates = v[\"candidates\"]\n",
    "                        if \"program\" in v[\"best_udf_types\"]:\n",
    "                            for best_udf_id in v[\"best_udf_ids\"]:\n",
    "                                if candidates[best_udf_id][\"udf_type\"] == \"program\":\n",
    "                                    for p in candidates[best_udf_id][\"program_types\"]:\n",
    "                                        best_program_types_when_best_is_program[p] += 1\n",
    "                                        best_program_types_when_best_is_program[\"base\"] += 1\n",
    "                            for udf_id, udf_dict in candidates.items():\n",
    "                                if udf_dict[\"udf_type\"] == \"program\":\n",
    "                                    f1_scores_when_best_is_program.append(udf_dict[\"test_score\"])\n",
    "                            best_f1_scores_when_best_is_program.append(v[\"best_test_score\"])\n",
    "                        else:\n",
    "                            best_program_score = -1\n",
    "                            best_program_types = []\n",
    "                            for udf_id, udf_dict in candidates.items():\n",
    "                                if udf_dict[\"udf_type\"] == \"program\":\n",
    "                                    f1_scores_when_best_is_not_program.append(udf_dict[\"test_score\"])\n",
    "                                    if udf_dict[\"test_score\"] > best_program_score:\n",
    "                                        best_program_score = udf_dict[\"test_score\"]\n",
    "                                        best_program_types = udf_dict[\"program_types\"]\n",
    "                                        best_program_types.append(\"base\")\n",
    "                                    elif udf_dict[\"test_score\"] == best_program_score:\n",
    "                                        best_program_types.extend(udf_dict[\"program_types\"])\n",
    "                                        best_program_types.append(\"base\")\n",
    "                            for p in best_program_types:\n",
    "                                best_program_types_when_best_is_not_program[p] += 1\n",
    "                            best_f1_scores_when_best_is_not_program.append(best_program_score)\n",
    "                except Exception as e:\n",
    "                    print(f\"Error: {e}, query_class_name={query_class_name}, question_id={question_id}, run_id={run_id}\")\n",
    "    print(\"[when best is program]\")\n",
    "    print(\"best_program_types\")\n",
    "    for k, v in sorted(best_program_types_when_best_is_program.items(), key=lambda x: -x[1]):\n",
    "        print(f\"{k}: {v}\")\n",
    "    print(f\"best_f1_scores: 25 percentile={np.percentile(best_f1_scores_when_best_is_program, 25)}, 50 percentile={np.percentile(best_f1_scores_when_best_is_program, 50)}, 75 percentile={np.percentile(best_f1_scores_when_best_is_program, 75)}\")\n",
    "    print(f\"f1_scores: 25 percentile={np.percentile(f1_scores_when_best_is_program, 25)}, 50 percentile={np.percentile(f1_scores_when_best_is_program, 50)}, 75 percentile={np.percentile(f1_scores_when_best_is_program, 75)}\")\n",
    "    print()\n",
    "    print(\"[when best is not program]\")\n",
    "    print(\"best_program_types\")\n",
    "    for k, v in sorted(best_program_types_when_best_is_not_program.items(), key=lambda x: -x[1]):\n",
    "        print(f\"{k}: {v}\")\n",
    "    print(f\"best_f1_scores: 25 percentile={np.percentile(best_f1_scores_when_best_is_not_program, 25)}, 50 percentile={np.percentile(best_f1_scores_when_best_is_not_program, 50)}, 75 percentile={np.percentile(best_f1_scores_when_best_is_not_program, 75)}\")\n",
    "    print(f\"f1_scores: 25 percentile={np.percentile(f1_scores_when_best_is_not_program, 25)}, 50 percentile={np.percentile(f1_scores_when_best_is_not_program, 50)}, 75 percentile={np.percentile(f1_scores_when_best_is_not_program, 75)}\")\n",
    "    print()\n",
    "    print()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clevrer\n",
      "[when best is program]\n",
      "best_program_types\n",
      "base: 147\n",
      "reuse: 63\n",
      "parameter: 57\n",
      "pixel: 27\n",
      "best_f1_scores: 25 percentile=0.869850132912086, 50 percentile=0.981864192323914, 75 percentile=0.9995992787016631\n",
      "f1_scores: 25 percentile=0.014322657648697038, 50 percentile=0.44379099751187506, 75 percentile=0.7430984754841368\n",
      "\n",
      "[when best is not program]\n",
      "best_program_types\n",
      "base: 205\n",
      "reuse: 146\n",
      "parameter: 104\n",
      "pixel: 50\n",
      "best_f1_scores: 25 percentile=0.11410133808082869, 50 percentile=0.5180923542770628, 75 percentile=0.6045581895882516\n",
      "f1_scores: 25 percentile=0.0, 50 percentile=0.0, 75 percentile=0.1281340276139313\n",
      "\n",
      "\n",
      "CityFlow\n",
      "[when best is program]\n",
      "best_program_types\n",
      "base: 24\n",
      "parameter: 15\n",
      "reuse: 9\n",
      "best_f1_scores: 25 percentile=0.9595330739299611, 50 percentile=1.0, 75 percentile=1.0\n",
      "f1_scores: 25 percentile=0.33152901023890785, 50 percentile=0.6793912819190095, 75 percentile=0.8236407532343673\n",
      "\n",
      "[when best is not program]\n",
      "best_program_types\n",
      "base: 241\n",
      "reuse: 215\n",
      "parameter: 77\n",
      "best_f1_scores: 25 percentile=0.30133897026621237, 50 percentile=0.35876705406771103, 75 percentile=0.4155177941647964\n",
      "f1_scores: 25 percentile=0.0, 50 percentile=0.14202912687651803, 75 percentile=0.30190796857463525\n",
      "\n",
      "\n",
      "Charades\n",
      "[when best is program]\n",
      "best_program_types\n",
      "base: 72\n",
      "parameter: 62\n",
      "reuse: 10\n",
      "best_f1_scores: 25 percentile=0.7033312657051359, 50 percentile=0.9983572962033618, 75 percentile=1.0\n",
      "f1_scores: 25 percentile=0.004569420035149385, 50 percentile=0.08030082819017642, 75 percentile=0.5101311706161643\n",
      "\n",
      "[when best is not program]\n",
      "best_program_types\n",
      "base: 200\n",
      "parameter: 140\n",
      "reuse: 35\n",
      "best_f1_scores: 25 percentile=0.04287322754301397, 50 percentile=0.17247237856993952, 75 percentile=0.5428760518105323\n",
      "f1_scores: 25 percentile=0.0, 50 percentile=0.008880994671403197, 75 percentile=0.08111019733522401\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Clevrer\n",
    "print(\"Clevrer\")\n",
    "dataset = \"clevrer\"\n",
    "query_class_names = [\n",
    "    \"3_new_udfs_labels\",\n",
    "]\n",
    "question_ids = list(range(30))\n",
    "run_ids = list(range(3))\n",
    "vocal_udf_config_name = \"ninterp=10-nparams=5-kwargs=True-pixels=True-pretrained_models=False-ntrain_distill=100-nselection_samples=500-selection=both-labels=user-budget=20-llm_method=gpt4v\"\n",
    "eval_program_udfs(dataset, query_class_names, question_ids, run_ids, vocal_udf_config_name)\n",
    "\n",
    "# CityFlow\n",
    "print(\"CityFlow\")\n",
    "dataset = \"cityflow\"\n",
    "query_class_names = [\n",
    "    \"unavailable_pred=1-unavailable_attr_pred=1-npred=1-nattr_pred=2-nvars=3-depth=3-max_duration=15-min_npos=74-max_npos=737\",\n",
    "    \"unavailable_pred=1-unavailable_attr_pred=1-npred=2-nattr_pred=2-nvars=3-depth=3-max_duration=15-min_npos=74-max_npos=737\"\n",
    "]\n",
    "question_ids = list(range(15))\n",
    "run_ids = list(range(3))\n",
    "vocal_udf_config_name = \"ninterp=10-nparams=5-kwargs=True-pixels=False-pretrained_models=False-ntrain_distill=500-nselection_samples=500-selection=both-labels=user-budget=50-llm_method=gpt4v\"\n",
    "eval_program_udfs(dataset, query_class_names, question_ids, run_ids, vocal_udf_config_name)\n",
    "\n",
    "# Charades\n",
    "print(\"Charades\")\n",
    "dataset = \"charades\"\n",
    "query_class_names = [\n",
    "    \"unavailable=2-npred=4-nobj_pred=1-nvars=3-depth=2\",\n",
    "    \"unavailable=2-npred=4-nobj_pred=1-nvars=2-depth=2\",\n",
    "    \"unavailable=2-npred=3-nobj_pred=1-nvars=2-depth=2\"\n",
    "]\n",
    "question_ids = list(range(10))\n",
    "run_ids = list(range(3))\n",
    "vocal_udf_config_name = \"ninterp=10-nparams=5-kwargs=True-pixels=False-pretrained_models=False-ntrain_distill=500-nselection_samples=500-selection=both-labels=user-budget=50-llm_method=gpt4v\"\n",
    "eval_program_udfs(dataset, query_class_names, question_ids, run_ids, vocal_udf_config_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Distilled-model UDFs "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_model_udfs(dataset, query_class_names, question_ids, run_ids, vocal_udf_config_name):\n",
    "    \"\"\" Only evaluate correctly proposed UDFs\"\"\"\n",
    "    f1_scores_when_best_is_model = []\n",
    "    f1_scores_when_best_is_not_model = []\n",
    "    dummy_f1_scores_when_best_is_model = []\n",
    "    dummy_f1_scores_when_best_is_not_model = []\n",
    "\n",
    "    for query_class_name in query_class_names:\n",
    "        for run_id in run_ids:\n",
    "            for question_id in question_ids:\n",
    "                try:\n",
    "                    input_query_file = os.path.join(config[\"data_dir\"], dataset, f\"{query_class_name}.json\")\n",
    "                    input_query = json.load(open(input_query_file, \"r\"))[\"questions\"][question_id]\n",
    "                    new_modules = input_query[\"new_modules\"]\n",
    "                    gt_udfs = new_modules\n",
    "\n",
    "                    with open(os.path.join(config['output_dir'], \"best_udf_type\", dataset, query_class_name, vocal_udf_config_name, f\"qid={question_id}-run={run_id}.json\"), \"r\") as f:\n",
    "                        best_udf_type_data = json.load(f)\n",
    "                    for udf_name, v in best_udf_type_data.items():\n",
    "                        if not is_correctly_proposed(dataset, udf_name, gt_udfs):\n",
    "                            continue\n",
    "                        candidates = v[\"candidates\"]\n",
    "                        if \"model\" in v[\"best_udf_types\"]:\n",
    "                            f1_scores_when_best_is_model.append(v[\"best_test_score\"])\n",
    "                            for udf_id, udf_dict in candidates.items():\n",
    "                                if udf_dict[\"udf_type\"] == \"dummy\":\n",
    "                                    dummy_f1_scores_when_best_is_model.append(udf_dict[\"test_score\"])\n",
    "                                    break\n",
    "                        else:\n",
    "                            for udf_id, udf_dict in candidates.items():\n",
    "                                if udf_dict[\"udf_type\"] == \"model\":\n",
    "                                    f1_scores_when_best_is_not_model.append(udf_dict[\"test_score\"])\n",
    "                                elif udf_dict[\"udf_type\"] == \"dummy\":\n",
    "                                    dummy_f1_scores_when_best_is_not_model.append(udf_dict[\"test_score\"])\n",
    "\n",
    "                except Exception as e:\n",
    "                    print(f\"Error: {e}, query_class_name={query_class_name}, question_id={question_id}, run_id={run_id}\")\n",
    "    print(\"[when best is model]\")\n",
    "    print(f\"f1_scores: 25 percentile={np.percentile(f1_scores_when_best_is_model, 25)}, 50 percentile={np.percentile(f1_scores_when_best_is_model, 50)}, 75 percentile={np.percentile(f1_scores_when_best_is_model, 75)}\")\n",
    "    print(f\"dummy_f1_scores: 25 percentile={np.percentile(dummy_f1_scores_when_best_is_model, 25)}, 50 percentile={np.percentile(dummy_f1_scores_when_best_is_model, 50)}, 75 percentile={np.percentile(dummy_f1_scores_when_best_is_model, 75)}\")\n",
    "    print()\n",
    "\n",
    "    print(\"[when best is not model]\")\n",
    "    print(f\"f1_scores: 25 percentile={np.percentile(f1_scores_when_best_is_not_model, 25)}, 50 percentile={np.percentile(f1_scores_when_best_is_not_model, 50)}, 75 percentile={np.percentile(f1_scores_when_best_is_not_model, 75)}\")\n",
    "    print(f\"dummy_f1_scores: 25 percentile={np.percentile(dummy_f1_scores_when_best_is_not_model, 25)}, 50 percentile={np.percentile(dummy_f1_scores_when_best_is_not_model, 50)}, 75 percentile={np.percentile(dummy_f1_scores_when_best_is_not_model, 75)}\")\n",
    "    print()\n",
    "    print()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clevrer\n",
      "[when best is model]\n",
      "f1_scores: 25 percentile=0.8032120824727298, 50 percentile=0.8444200424870745, 75 percentile=0.898361855256242\n",
      "dummy_f1_scores: 25 percentile=0.2278144417381927, 50 percentile=0.5209288566780063, 75 percentile=0.5253999852539999\n",
      "\n",
      "[when best is not model]\n",
      "f1_scores: 25 percentile=0.3155300839511366, 50 percentile=0.47207903431838627, 75 percentile=0.6405950814644101\n",
      "dummy_f1_scores: 25 percentile=0.23352764529235118, 50 percentile=0.6643958050656311, 75 percentile=0.6676437279328492\n",
      "\n",
      "\n",
      "CityFlow\n",
      "[when best is model]\n",
      "f1_scores: 25 percentile=0.5497728333482863, 50 percentile=0.653390668057452, 75 percentile=0.7927347903130537\n",
      "dummy_f1_scores: 25 percentile=0.27912579590431946, 50 percentile=0.28237718996908273, 75 percentile=0.30966869506423256\n",
      "\n",
      "[when best is not model]\n",
      "f1_scores: 25 percentile=0.6681438421324796, 50 percentile=0.694771797129113, 75 percentile=0.7325547845858325\n",
      "dummy_f1_scores: 25 percentile=0.6627440492110189, 50 percentile=0.667732480682121, 75 percentile=0.6772486772486772\n",
      "\n",
      "\n",
      "Charades\n",
      "[when best is model]\n",
      "f1_scores: 25 percentile=0.1107011070110701, 50 percentile=0.20370370370370372, 75 percentile=0.7228360957642727\n",
      "dummy_f1_scores: 25 percentile=0.01744647105471848, 50 percentile=0.02410590792333531, 75 percentile=0.19624819624819623\n",
      "\n",
      "[when best is not model]\n",
      "f1_scores: 25 percentile=0.30946779419279125, 50 percentile=0.36170612551672304, 75 percentile=0.6085143635673407\n",
      "dummy_f1_scores: 25 percentile=0.5876703622625521, 50 percentile=0.721064074689858, 75 percentile=0.9288774635818338\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Clevrer\n",
    "print(\"Clevrer\")\n",
    "dataset = \"clevrer\"\n",
    "query_class_names = [\n",
    "    \"3_new_udfs_labels\",\n",
    "]\n",
    "question_ids = list(range(30))\n",
    "run_ids = list(range(3))\n",
    "vocal_udf_config_name = \"ninterp=10-nparams=5-kwargs=True-pixels=True-pretrained_models=False-ntrain_distill=100-nselection_samples=500-selection=both-labels=user-budget=20-llm_method=gpt4v\"\n",
    "eval_model_udfs(dataset, query_class_names, question_ids, run_ids, vocal_udf_config_name)\n",
    "\n",
    "# CityFlow\n",
    "print(\"CityFlow\")\n",
    "dataset = \"cityflow\"\n",
    "query_class_names = [\n",
    "    \"unavailable_pred=1-unavailable_attr_pred=1-npred=1-nattr_pred=2-nvars=3-depth=3-max_duration=15-min_npos=74-max_npos=737\",\n",
    "    \"unavailable_pred=1-unavailable_attr_pred=1-npred=2-nattr_pred=2-nvars=3-depth=3-max_duration=15-min_npos=74-max_npos=737\"\n",
    "]\n",
    "question_ids = list(range(15))\n",
    "run_ids = list(range(3))\n",
    "vocal_udf_config_name = \"ninterp=10-nparams=5-kwargs=True-pixels=False-pretrained_models=False-ntrain_distill=500-nselection_samples=500-selection=both-labels=user-budget=50-llm_method=gpt4v\"\n",
    "eval_model_udfs(dataset, query_class_names, question_ids, run_ids, vocal_udf_config_name)\n",
    "\n",
    "# Charades\n",
    "print(\"Charades\")\n",
    "dataset = \"charades\"\n",
    "query_class_names = [\n",
    "    \"unavailable=2-npred=4-nobj_pred=1-nvars=3-depth=2\",\n",
    "    \"unavailable=2-npred=4-nobj_pred=1-nvars=2-depth=2\",\n",
    "    \"unavailable=2-npred=3-nobj_pred=1-nvars=2-depth=2\"\n",
    "]\n",
    "question_ids = list(range(10))\n",
    "run_ids = list(range(3))\n",
    "vocal_udf_config_name = \"ninterp=10-nparams=5-kwargs=True-pixels=False-pretrained_models=False-ntrain_distill=500-nselection_samples=500-selection=both-labels=user-budget=50-llm_method=gpt4v\"\n",
    "eval_model_udfs(dataset, query_class_names, question_ids, run_ids, vocal_udf_config_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Labeling quality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset=cityflow\n",
      "[gpt4v] udf_name=suv, mean=0.782, std=0.012\n",
      "[gpt4v] udf_name=white, mean=0.883, std=0.008\n",
      "[gpt4v] udf_name=grey, mean=0.762, std=0.014\n",
      "[gpt4v] udf_name=van, mean=0.872, std=0.012\n",
      "[gpt4v] udf_name=sedan, mean=0.797, std=0.008\n",
      "[gpt4v] udf_name=black, mean=0.784, std=0.011\n",
      "[gpt4v] udf_name=red, mean=0.801, std=0.012\n",
      "[gpt4v] udf_name=blue, mean=0.874, std=0.007\n",
      "[gpt4v] udf_name=pickup_truck, mean=0.912, std=0.005\n",
      "\n",
      "dataset=charades\n",
      "[gpt4v] udf_name=holding, mean=0.563, std=0.012\n",
      "[gpt4v] udf_name=sitting_on, mean=0.781, std=0.005\n",
      "[gpt4v] udf_name=standing_on, mean=0.868, std=0.005\n",
      "[gpt4v] udf_name=covered_by, mean=0.717, std=0.009\n",
      "[gpt4v] udf_name=carrying, mean=0.718, std=0.012\n",
      "[gpt4v] udf_name=eating, mean=0.547, std=0.036\n",
      "[gpt4v] udf_name=wiping, mean=0.671, std=0.017\n",
      "[gpt4v] udf_name=touching, mean=0.536, std=0.022\n",
      "[gpt4v] udf_name=leaning_on, mean=0.756, std=0.020\n",
      "[gpt4v] udf_name=wearing, mean=0.820, std=0.014\n",
      "[gpt4v] udf_name=drinking_from, mean=0.574, std=0.016\n",
      "[gpt4v] udf_name=lying_on, mean=0.804, std=0.011\n",
      "[gpt4v] udf_name=writing_on, mean=0.803, std=0.017\n",
      "[gpt4v] udf_name=above, mean=0.356, std=0.010\n",
      "[gpt4v] udf_name=in_front_of, mean=0.643, std=0.005\n",
      "[gpt4v] udf_name=beneath, mean=0.310, std=0.019\n",
      "[gpt4v] udf_name=behind, mean=0.606, std=0.016\n",
      "[gpt4v] udf_name=in, mean=0.483, std=0.008\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def eval_labeling_quality(dataset):\n",
    "    if dataset == \"cityflow\":\n",
    "        udfs = [\"suv\", \"white\", \"grey\", \"van\", \"sedan\", \"black\", \"red\", \"blue\", \"pickup_truck\"]\n",
    "    elif dataset == \"charades\":\n",
    "        # removing \"have_it_on_the_back\" and \"twisting\" due to insufficient positives\n",
    "        udfs = [\"holding\", \"sitting_on\", \"standing_on\", \"covered_by\", \"carrying\", \"eating\", \"wiping\", \"touching\", \"leaning_on\", \"wearing\", \"drinking_from\", \"lying_on\", \"writing_on\", \"above\", \"in_front_of\", \"beneath\", \"behind\", \"in\"]\n",
    "    else:\n",
    "        raise ValueError(f\"dataset={dataset}\")\n",
    "\n",
    "    gpt4v_results = defaultdict(list)\n",
    "    for udf_name in udfs:\n",
    "        for run_id in range(3):\n",
    "            # random.seed(run_id)\n",
    "            # np.random.seed(run_id)\n",
    "            try:\n",
    "                with open(os.path.join(config[\"log_dir\"], \"labeling_quality\", dataset, \"balanced=True\", f\"udf_name={udf_name}-n_train_distill=500-llm_method=gpt4v-run_id={run_id}.log\"), \"r\") as f:\n",
    "                    lines = f.readlines()\n",
    "                    llm_f1 = -1\n",
    "                    npos, nneg = 0, 0\n",
    "                    for line in lines:\n",
    "                        if \"llm_f1: \" in line:\n",
    "                            # 2024-07-21 23:37:06,240 - vocaludf - DEBUG - llm_TP: 63, llm_FP: 47, llm_TN: 203, llm_FN: 187, llm_f1: 0.35\n",
    "                            pattern = r\"llm_TP: (\\d+), llm_FP: (\\d+), llm_TN: (\\d+), llm_FN: (\\d+), llm_f1: ([\\d.]+)\"\n",
    "                            match = re.search(pattern, line)\n",
    "                            llm_tp = int(match.group(1))\n",
    "                            llm_fp = int(match.group(2))\n",
    "                            llm_tn = int(match.group(3))\n",
    "                            llm_fn = int(match.group(4))\n",
    "                            llm_f1 = float(match.group(5))\n",
    "                            npos = llm_tp + llm_fn\n",
    "                            nneg = llm_fp + llm_tn\n",
    "                            # print(f\"llm_f1={llm_f1}, npos={npos}, nneg={nneg}\")\n",
    "                            # if npos != nneg:\n",
    "                            #     print(f\"not enough positives: udf_name={udf_name}, run_id={run_id}\")\n",
    "                            break\n",
    "                    if llm_f1 == -1:\n",
    "                        print(f\"failed task: udf_name={udf_name}, run_id={run_id}\")\n",
    "                        llm_f1 = 0\n",
    "                    gpt4v_results[udf_name].append(llm_f1)\n",
    "            except Exception as e:\n",
    "                print(f\"Error: {e}, udf_name={udf_name}, run_id={run_id}\")\n",
    "\n",
    "    print(f\"dataset={dataset}\")\n",
    "    for udf_name, llm_f1s in gpt4v_results.items():\n",
    "        print(f\"[gpt4v] udf_name={udf_name}, mean={np.mean(llm_f1s):.3f}, std={np.std(llm_f1s):.3f}\")\n",
    "    print()\n",
    "\n",
    "eval_labeling_quality(\"cityflow\")\n",
    "eval_labeling_quality(\"charades\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "jupyter",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
